Analysis with a large number of variables generally requires a large amount of memory and computation power; also it may cause a classification algorithm to overfit. Large amount of data is also suspected to be redundant (e.g. the same measurement in both feet and meters), then it can be transformed into a reduced set of features (also named a feature vector (on example) or feature matrix (multiple examples)). Feature selection techniques should be distinguished from feature extraction. Feature extraction creates new features from functions of the original data/features, whereas feature selection returns a subset of the features.                                    The probe variable : With this option, the number of selected features is determined using random probe variables. We work with the assumption that features contain a combination of information and noise. We draw a set of random features (termed as probe variables) whose ranking is compared with the real features. We run the feature ranking method, and stop when "probe probability" (box on the right, between 0 and 1 for 0-100%) probes have been ranked higher than real variables. In other words, we stop the ranking when the features have a risk of "probe probability" to contain no more information than random noise. In SIGMA, user can choose between selecting the Nth best features (“Number of feature” in Feature Ranking Method”) or the subset of features that performs better than a random variable included in the whole matrix feature set, with a given probability (“Probe variable”) that the user can set. The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data.