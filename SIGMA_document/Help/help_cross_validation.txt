Cross-validation is a technique that is used for the assessment of how the results of statistical analysis generalize to an independent data set. The holdout method is the simplest kind of cross validation. A round of cross-validation comprises the partitioning of data into complementary subsets, called the training set and the testing set. The test data is removed before training begins, then when training is done, the data that was removed can be used to test the performance of the learned/trained model. This model is asked to predict the output values for the data in the testing set (it has never seen these output values before). A complete and efficient model is based on a multiple rounds of cross-validation using many different partitions and then an average of the results are taken. Cross-validation is a powerful technique in the estimation of model performance technique. In this version of SIGMA, the ‘Leave-One-Out’ cross validation is implemented. This is based on two approaches. 
LOSO : Leave One Subject Out Ths method train a model using all the subject except one and try to predict the outputs of the leaved out subject. A subject is supposed the have more than one example, else the LOSO will be the same as the LOEO. The process of the LOSO is repeated n times util the subject will be all processed.  LOEO : Leave One Epoch/Example Out Leave only one example out and train with all the rest of the example. When the training is over, the prediction is computed on the one leaved example. This process is repeated m times until the examples are all used. Note : In the case of custom feature classification, only the LOEO should used.  The cross validation method ensure that the model is able to fit the data in a good manner and generalize the model to be able to do predict new data sets. Others cross validation methods will be added for the future version of SIGMA.